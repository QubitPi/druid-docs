"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[9980],{28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>i});var o=n(96540);const s={},r=o.createContext(s);function a(e){const t=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(r.Provider,{value:t},e.children)}},66340:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>i,default:()=>l,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"development/extensions-core/parquet","title":"Apache Parquet Extension","description":"\x3c!--","source":"@site/docs/latest/development/extensions-core/parquet.md","sourceDirName":"development/extensions-core","slug":"/development/extensions-core/parquet","permalink":"/docs/latest/development/extensions-core/parquet","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"parquet","title":"Apache Parquet Extension"}}');var s=n(74848),r=n(28453);const a={id:"parquet",title:"Apache Parquet Extension"},i=void 0,d={},c=[];function p(e){const t={a:"a",code:"code",p:"p",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["This Apache Druid module extends ",(0,s.jsx)(t.a,{href:"/docs/latest/ingestion/hadoop",children:"Druid Hadoop based indexing"})," to ingest data directly from offline\nApache Parquet files."]}),"\n",(0,s.jsxs)(t.p,{children:["Note: If using the ",(0,s.jsx)(t.code,{children:"parquet-avro"})," parser for Apache Hadoop based indexing, ",(0,s.jsx)(t.code,{children:"druid-parquet-extensions"})," depends on the ",(0,s.jsx)(t.code,{children:"druid-avro-extensions"})," module, so be sure to\n",(0,s.jsx)(t.a,{href:"/docs/latest/configuration/extensions#loading-extensions",children:"include  both"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.code,{children:"druid-parquet-extensions"})," provides the ",(0,s.jsx)(t.a,{href:"/docs/latest/ingestion/data-formats#parquet",children:"Parquet input format"}),", the ",(0,s.jsx)(t.a,{href:"/docs/latest/ingestion/data-formats#parquet-hadoop-parser",children:"Parquet Hadoop parser"}),",\nand the ",(0,s.jsx)(t.a,{href:"/docs/latest/ingestion/data-formats#parquet-avro-hadoop-parser",children:"Parquet Avro Hadoop Parser"})," with ",(0,s.jsx)(t.code,{children:"druid-avro-extensions"}),".\nThe Parquet input format is available for ",(0,s.jsx)(t.a,{href:"/docs/latest/ingestion/native-batch",children:"native batch ingestion"}),"\nand the other 2 parsers are for ",(0,s.jsx)(t.a,{href:"/docs/latest/ingestion/hadoop",children:"Hadoop batch ingestion"}),".\nPlease see corresponding docs for details."]})]})}function l(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}}}]);